{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7whhErFzY9J"
      },
      "source": [
        "# <font color=\"steelblue\">Algoritmos Bagging avanzados: Bosques aleatorios (Random Forest)</font>\n",
        "\n",
        "**Autor√≠a**: \n",
        "\n",
        "*   Fernando Borr√°s (f.borras@umh.es)\n",
        "*   Federico Botella (federico@umh.es)\n",
        "*   In√©s Hern√°ndez (ines.hernandezp@umh.es)\n",
        "*   M¬™ Asunci√≥n Mart√≠nez Mayoral (asun.mayoral@umh.es)\n",
        "*   Josep Molt√≥ (j.molto@umh.es)\n",
        "*   Javier Morales (j.morales@umh.es) \n",
        "\n",
        "Departamento de Estad√≠stica, Matem√°ticas e Inform√°tica. \n",
        "\n",
        "Universidad Miguel Hern√°ndez de Elche. \n",
        "\n",
        "\n",
        "**Financiaci√≥n**: El material que aparece a continuaci√≥n se ha desarrollado dentro del marco del proyecto UNIDIGITAL- IASAC.\n",
        "\n",
        "<small><img src=https://raw.githubusercontent.com/ia4legos/MachineLearning/main/images/IASAC-UMH.png width=\"450\" height=\"200\"></small>\n",
        "\n",
        "**Fecha √∫ltima edici√≥n**: 26/05/2022\n",
        "\n",
        "**Licencia**: <small><a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br /></small>\n",
        "\n",
        "No olvides hacer una copia si deseas utilizarlo. Al usar estos contenidos, acepta nuestros t√©rminos de uso y nuestra pol√≠tica de privacidad. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"steelblue\">Configuraci√≥n del cuaderno</font>\n",
        "\n",
        "Para garantizar la funcionalidad completa de este cuaderno, es preciso ejecutar la celda de c√≥digo a continuaci√≥n."
      ],
      "metadata": {
        "id": "O7Ih_r3AZeB0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCLJ7Ka9Rq35",
        "outputId": "b70f5427-556c-4122-8d2f-0314666182ab",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuaderno configurado\n"
          ]
        }
      ],
      "source": [
        "#@title <b><font color=\"steelblue\" size=\"+1\"> Configuraci√≥n de cuaderno\n",
        "\n",
        "# Cargamos m√≥dulos\n",
        "from io import StringIO \n",
        "import sys\n",
        "import numpy as np      # importamos numpy como np\n",
        "import pandas as pd     # importamos pandas como pd\n",
        "import math             # importamos m√≥dulo para c√°culos matem√°ticos\n",
        "\n",
        "# Esta l√≠nea configura matplotlib para mostrar las figuras incrustadas en el jupyter notebook\n",
        "# Configuraci√≥nde entorno gr√°fico\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt # importamos matplotlib como plt\n",
        "import seaborn as sns # importamos seaborn como sns\n",
        "sns.set_style(\"ticks\")\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "class Capturing(list):\n",
        "    def __enter__(self):\n",
        "        self._stdout = sys.stdout\n",
        "        sys.stdout = self._stringio = StringIO()\n",
        "        return self\n",
        "    def __exit__(self, *args):\n",
        "        self.extend(self._stringio.getvalue().splitlines())\n",
        "        del self._stringio    # free up some memory\n",
        "        sys.stdout = self._stdout\n",
        "\n",
        "with Capturing() as output:\n",
        "    print('Comenzamos....')\n",
        "\n",
        "with Capturing(output) as output:\n",
        "    # Librer√≠as\n",
        "    import os\n",
        "    !pip install jupyterquiz\n",
        "    from jupyterquiz import display_quiz\n",
        "    import json\n",
        "    import base64\n",
        "    # Lectura ficheros json\n",
        "    # A configurar pra cada cuaderno en funci√≥n de las preguntas de autoevalauci√≥n\n",
        "    for i in range(1,7):\n",
        "      !wget {f\"https://raw.githubusercontent.com/ia4legos/MachineLearning/main/autoeval/auto_50_{i}.json\"}\n",
        "    print(\"Cuaderno configurado\")\n",
        "\n",
        "if output[-1]=='Cuaderno configurado':\n",
        "    print(output[-1])\n",
        "else: \n",
        "    print(output[:-2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt99G2Jk5jX4"
      },
      "source": [
        "# <font color=\"steelblue\">Introducci√≥n</font>\n",
        "\n",
        "**Descripci√≥n:** \n",
        "\n",
        "**Nivel de Formaci√≥n:** \n",
        "\n",
        "**Recomendaciones antes de usarlo:** \n",
        "\n",
        "**Objetivos de aprendizaje:**\n",
        "\n",
        "* "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"steelblue\">Objetivos de aprendizaje</font>\n",
        "\n",
        "- "
      ],
      "metadata": {
        "id": "F4EcPF7MZ0pY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"steelblue\">Contenidos</font>\n",
        "\n",
        "1. "
      ],
      "metadata": {
        "id": "DsQSZxb8Z6-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"steelblue\">Introducci√≥n al bosque aleatorio (Random Forest)</font>"
      ],
      "metadata": {
        "id": "QAgaWSzDmSw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un modelo de **bosque aleatorio** est√° formado por un conjunto de √°rboles de decisi√≥n individuales, cada uno entrenado con una muestra ligeramente distinta de los datos de entrenamiento generada mediante bootstrapping. La predicci√≥n de una nueva observaci√≥n se obtiene agregando las predicciones de todos los √°rboles individuales que forman el modelo.\n",
        "\n",
        "Muchos m√©todos predictivos generan modelos globales en los que disponemos de una √∫nica ecuaci√≥n o modelo de predicci√≥n. Sin embargo, en situaciones complejas con m√∫ltiples predictores, que interaccionan entre ellos de forma compleja y no lineal, es muy dif√≠cil encontrar un modelo predictivo los suficientemente preciso. Como ya hemos visto anteriormente los √°rboles de decisi√≥n nos permiten obtener un modelo con el que odemos manejar de forma sencilla relaciones complejas entre las posibles predictoras\n",
        "\n",
        "Sin embargo, como ya hemos visto la utilizaci√≥n de los √°rboles de decisi√≥n no est√° exenta de dificultades y por ese motivo se introduce aqui el algortimo de bosque aleatorio que es un m√©todo de conjunto bagging que nos permite mejorar la capacidad predictiva de los √°rboles de decisi√≥n individuales.\n",
        "\n",
        "Entre la ventajas del suso de este tipo de algortimo podemos destacar:\n",
        "\n",
        "* Son capaces de seleccionar predictores de forma autom√°tica.\n",
        "* Pueden aplicarse a problemas de regresi√≥n y clasificaci√≥n.\n",
        "* Al tratarse de m√©todos no param√©tricos, no es necesario que se cumpla ning√∫n tipo de distribuci√≥n espec√≠fica.\n",
        "* Por lo general, requieren mucha menos limpieza y pre procesado de los datos en comparaci√≥n a otros m√©todos de aprendizaje estad√≠stico (por ejemplo, no requieren estandarizaci√≥n).\n",
        "* No se ven muy influenciados por outliers.\n",
        "* Son muy √∫tiles en la exploraci√≥n de datos, permiten identificar de forma r√°pida y eficiente las variables (predictores) m√°s importantes.\n",
        "* Gracias al Out-of-Bag Error puede estimarse su error de validaci√≥n sin necesidad de recurrir a estrategias computacionalmente costosas como la validaci√≥n cruzada. \n",
        "\n",
        "Entre las desvantajas podemos destacar:\n",
        "\n",
        "* Al combinar m√∫ltiples √°rboles, se pierde la interpretabilidad que tienen los modelos basados en un √∫nico √°rbol.\n",
        "* Cuando tratan con predictores continuos, pierden parte de su informaci√≥n al categorizarlas en el momento de la divisi√≥n de los nodos.\n",
        "* Por la forma de construcci√≥n de los √°rboles de decisi√≥n los predictores continuos o predictores cualitativos con muchos niveles tienen mayor probabilidad de contener, solo por azar, alg√∫n punto de corte √≥ptimo, por lo que suelen verse favorecidos en la creaci√≥n de los √°rboles.\n",
        "* No son capaces de extrapolar fuera del rango de los predictores observado en los datos de entrenamiento."
      ],
      "metadata": {
        "id": "sylDxIgKNxgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"steelblue\">Algoritmo de bosque aleatorio</font>\n",
        "\n",
        "Antes de presentar el algoritmos espec√≠fico de bosque aleatorio es necesario conocer como funciona el proceso de bagging para un √∫nico √°rbol de decisi√≥n. Dicho algoritmo se organiza en tres pasos:\n",
        "\n",
        "1. Generar  ùêµ  pseudo-training sets mediante bootstrapping a partir de la muestra de entrenamiento original.\n",
        "2. Entrenar un √°rbol con cada una de las  ùêµ  muestras del paso 1. Cada √°rbol se crea sin apenas restricciones y no se somete a pruning, por lo que tiene varianza alta pero poco sesgo. En la mayor√≠a de casos, la √∫nica regla de parada es el n√∫mero m√≠nimo de observaciones que deben tener los nodos terminales. El valor √≥ptimo de este hiperpar√°metro puede obtenerse comparando el out of bag error o por validaci√≥n cruzada.\n",
        "3. Para cada una de la muestras de validaci√≥n, se obtiene la predicci√≥n en cada uno de los ùêµ  √°rboles. El valor final de la predicci√≥n se obtiene como la media de las  ùêµ  predicciones en el caso de variables cuantitativas y como la clase predicha m√°s frecuente (moda) para variables cualitativas.\n",
        "\n",
        "En el algoritmo descrito, el n√∫mero de √°rboles creados no es un hiperpar√°metro cr√≠tico en cuanto a que, por mucho que se incremente el n√∫mero, no se aumenta el riesgo de overfitting. Alcanzado un determinado n√∫mero de √°rboles, la reducci√≥n de test error se estabiliza. A pesar de ello, cada √°rbol ocupa memoria, por lo que no conviene almacenar m√°s de los necesarios.\n",
        "\n",
        "El algoritmo de Random Forest es una modificaci√≥n del proceso de bagging anterior que consigue mejorar los resultados gracias a que considera √°rboles lo m√°s independientes posible. \n",
        "\n",
        "Sup√≥ngase un set de datos en el que hay un predictor muy influyente, junto con otros moderadamente influyentes. En este escenario, todos o casi todos los √°rboles creados en el proceso de bagging estar√°n dominados por el mismo predictor y ser√°n muy parecidos entre ellos. Como consecuencia de la alta correlaci√≥n entre los √°rboles, el proceso de bagging apenas conseguir√° disminuir la varianza y, por lo tanto, tampoco mejorar el modelo. Random forest evita este problema haciendo una selecci√≥n aleatoria de  $ùëö$  predictores antes de evaluar cada divisi√≥n. De esta forma, un promedio de  $(ùëù‚àíùëö)/ùëù$  divisiones no contemplar√°n el predictor influyente, permitiendo que otros predictores puedan ser seleccionados. A√±adiendo este paso extra se consigue decorrelacionar los √°rboles todav√≠a m√°s, con lo que su agregaci√≥n consigue una mayor reducci√≥n de la varianza. Algunas recomndaciones para la selecci√≥n de $m$ son:\n",
        "\n",
        "* La ra√≠z cuadrada del n√∫mero total de predictores para problemas de clasificaci√≥n:  \n",
        "\n",
        "$$m \\approx \\sqrt{p}$$\n",
        "\n",
        "* Un tercio del n√∫mero de predictores para problemas de regresi√≥n:\n",
        "\n",
        "$$m \\approx p/3$$  \n",
        "\n",
        "* Si los predictores est√°n muy correlacionados, valores peque√±os de  $ùëö$  consiguen mejores resultados."
      ],
      "metadata": {
        "id": "k6w4J7a_ZRIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"steelblue\">Predicci√≥n mediante bosque aleatorio</font>"
      ],
      "metadata": {
        "id": "nAyouPxJijef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para realizar la predicci√≥n de un bosque aleatorio utilizamos el principio de bagging, de forma que, una vez determinamos el nod terminal al que es asinado la observaci√≥n apredecir en cada uno de los √°rboles, utilizamos las observaciones contenidas en dicho nodo terminal para la predicci√≥n individidual de cada uno de ellos. Si estamos en un modeo de regresi√≥n obtnemos la media de todas las observaciones del nodo terminal en cada √°rbol, mientras que si estamos en un problema de clasificaci√≥n actuamos mediante el voto por mayor√≠a.\n",
        "\n",
        "Una vez obtenemos las predicciones individuales la predicci√≥n conjunta se obtiene a partir de la media o de la categor√≠a m√°s frecuente de todas ellas en funci√≥n de que estemos en un problema de predicci√≥n o clasificaci√≥n.\n",
        "\n",
        "Sin embargo, en los problemas de regresi√≥n la predicci√≥n de un √°rbol de regresi√≥n puede verse como una variante de vecinos cercanos en la que, solo las observaciones que forman parte del mismo nodo terminal que la observaci√≥n predicha, tienen influencia. Siguiendo esta aproximaci√≥n, la predicci√≥n del √°rbol se define como la media ponderada de todas las observaciones de entrenamiento, donde el peso de cada observaci√≥n depende √∫nicamente de si forma parte o no del mismo nodo terminal, es decir, definimos los pesos del √°rbol j como un vector de $n$ componentes donde cada una de las componenetes toma el valor $w_j = 1/n_j$  si la observaci√≥n peetenece al nodo terminal $j$ con $n_j$ observaciones, y 0 en otro caso. Para el bosque aletorio esto equivale a la media ponderada de todas las observaciones, empleando como pesos  la media de los vectores de pesos de los $M$ √°rboles considerados, es decir,\n",
        "\n",
        "$$\\hat{w}=\\frac{\\sum_{i=1}^{M}  w_i}{M}$$\n",
        "\n",
        "$$y_{pred} = \\sum_{i=1}^n \\hat{w}_i y_i$$"
      ],
      "metadata": {
        "id": "LuIj5gYclK3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"steelblue\">Importancia de los predictores</font>"
      ],
      "metadata": {
        "id": "u1VVGDf0s--d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si bien es cierto que el bosque aleatorio consigue mejorar la capacidad predictiva en comparaci√≥n a los modelos basados en un √∫nico √°rbol, esto tiene un coste asociado, la interpretabilidad del modelo se reduce. Al tratarse de una combinaci√≥n de m√∫ltiples √°rboles, no es posible obtener una representaci√≥n gr√°fica sencilla del modelo y no es inmediato identificar de forma visual que predictores son m√°s importantes. Sin embargo, se han desarrollado nuevas estrategias para cuantificar la importancia de los predictores que hacen de los modelos de bosque aleatorio una herramienta muy potente, no solo para predecir, sino tambi√©n para el an√°lisis exploratorio. Dos de estas medidas son: importancia por permutaci√≥n e impureza de nodos."
      ],
      "metadata": {
        "id": "he0PC7JDtLKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"steelblue\">Importancia por permutaci√≥n</font>"
      ],
      "metadata": {
        "id": "YZXvVcm9uCvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifica la influencia que tiene cada predictor sobre una determinada m√©trica de evaluaci√≥n del modelo (estimada por out-of-bag error o validaci√≥n cruzada). El valor asociado con cada predictor se obtiene de la siguiente forma:\n",
        "\n",
        "1. Crear el conjunto de √°rboles que forman el modelo.\n",
        "\n",
        "2. Calcular una determinada m√©trica de error (mse, classification error, ...). Este es el valor de referencia ($ùëíùëüùëü_0$).\n",
        "\n",
        "3. Para cada predictor $ùëó$:\n",
        "\n",
        "  * Permutar en todos los √°rboles del modelo los valores del predictor $ùëó$ manteniendo el resto constante.\n",
        "\n",
        "  * Recalcular la m√©trica tras la permutaci√≥n, ll√°mese ($ùëíùëüùëü_ùëó$).\n",
        "\n",
        "  * Calcular el incremento en la m√©trica debido a la permutaci√≥n del predictor $ùëó$\n",
        "\n",
        "$$\\%I_ùëó=100*\\frac{err_j-err_0}{err_0}$$\n",
        "\n",
        "Si el predictor permutado estaba contribuyendo al modelo, es de esperar que el modelo aumente su error, ya que se pierde la informaci√≥n que proporcionaba esa variable. El porcentaje en que se incrementa el error debido a la permutaci√≥n del predictor $ùëó$  puede interpretarse como la influencia que tiene $ùëó$  sobre el modelo. Algo que suele llevar a confusiones es el hecho de que este incremento puede resultar negativo. Si la variable no contribuye al modelo, es posible que, al reorganizarla aleatoriamente, solo por azar, se consiga mejorar ligeramente el modelo, por lo que  $(ùëíùëüùëü_ùëó‚àíùëíùëüùëü_0)$ es negativo. A modo general, se puede considerar que estas variables tiene una importancia pr√≥xima a cero.\n",
        "\n",
        "Aunque esta estrategia suele ser la m√°s recomendado, cabe tomar algunas precauciones en su interpretaci√≥n. Lo que cuantifican es la influencia que tienen los predictores sobre el modelo, no su relaci√≥n con la variable respuesta. ¬øPor qu√© es esto tan importante? Sup√≥ngase un escenario en el que se emplea esta estrategia con la finalidad de identificar qu√© predictores est√°n relacionados con el peso de una persona, y que dos de los predictores son: el √≠ndice de masa corporal (IMC) y la altura. Como IMC y altura est√°n muy correlacionados entre s√≠ (la informaci√≥n que aportan es redundante), cuando se permute uno de ellos, el impacto en el modelo ser√° m√≠nimo, ya que el otro aporta la misma informaci√≥n. Como resultado, estos predictores aparecer√°n como poco influyentes aun cuando realmente est√°n muy relacionados con la variable respuesta. Una forma de evitar problemas de este tipo es, siempre que se excluyan predictores de un modelo, comprobar el impacto que tiene en su capacidad predictiva."
      ],
      "metadata": {
        "id": "HjrquR6quNt8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"steelblue\">Incremento de la pureza de los nodos</font>"
      ],
      "metadata": {
        "id": "5m653uKJwHof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cuantifica el incremento total en la pureza de los nodos debido a divisiones en las que participa el predictor (promedio de todos los √°rboles). La forma de calcularlo es la siguiente: en cada divisi√≥n de los √°rboles, se registra el descenso conseguido en la medida empleada como criterio de divisi√≥n (√≠ndice Gini, mse entrop√≠a, ...). Para cada uno de los predictores, se calcula el descenso medio conseguido en el conjunto de √°rboles que forman el conjunto. Cuanto mayor sea este valor medio, mayor la contribuci√≥n del predictor en el modelo."
      ],
      "metadata": {
        "id": "9lDInvB8wOxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"steelblue\">Hiperpar√°metros relevantes en el bosque aleatorio</font>"
      ],
      "metadata": {
        "id": "pOX9FjMDybO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Del conjunto de hiperpar√°metros que se pueden modificar en el bosque aleatorio los dos m√°s interesantes son el n√∫mero de √°rboles considerados y el n√∫meo m√°ximo de predictoras usadas en la construcci√≥n de cada √°rbol. \n",
        "\n",
        "Lo habitual es proceder de forma individual estudiando la influencia de cada uno de los hiperpar√°metros respecto de la capacidad predictiva del modelo utilizando el out of bag score (aunque se puede configurar el algoritmo para utilizar otra). Esas curvas de influencia nos permiten deteminar la evoluci√≥n del error del modelo con respecto a es hiperpar√°metro y obtener as√≠ el conjunto √≥ptimo de valores. \n",
        "\n",
        "Sin embargo, aunque el an√°lisis individual de los hiperpar√°metros es √∫til para entender su impacto en el modelo e identificar rangos de inter√©s, la b√∫squeda final no debe hacerse de forma secuencial, ya que cada hiperpar√°metro interacciona con los dem√°s. Es preferible recurrir a grid search o random search para analizar varias combinaciones de hiperpar√°metros. Los dos m√©todos m√°s habituales son el grid search basado en el out of bag o el grid search basado en validaci√≥n cruzada."
      ],
      "metadata": {
        "id": "hHxDpD6TzFzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"steelblue\">Codificaci√≥n de predictoras cualitativas</font>\n"
      ],
      "metadata": {
        "id": "Shoiye0tZucA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los modelos basados en √°rboles de decisi√≥n, entre ellos Random Forest, son capaces de utilizar predictores categ√≥ricos en su forma natural sin necesidad de convertirlos en variables dummy mediante one hot encoding. Sin embargo, en la pr√°ctica, depende de la implementaci√≥n que tenga la librer√≠a o software utilizado. Esto tiene impacto directo en la estructura de los √°rboles generados y, en consecuencia, en los resultados predictivos del modelo y en la importancia calculada para los predictores.\n",
        "\n",
        "Entre las dificulatdes m√°s relvantes al utilizar one hot encoding se pueden destacar:\n",
        "\n",
        "* El entrenamiento de los modelos es mas costos cuando se aplica one hotencoding debido al aumento de dimensionalidad al crear las nuevas variables dummy, obligando a que el algoritmo tenga que analizar muchos m√°s puntos de divisi√≥n.\n",
        "* Al convertir una variable categ√≥rica en m√∫ltiples variables dummy su importancia queda diluida, dificultando que el modelo pueda aprender de ella y perdiendo as√≠ capacidad predictiva. Este efecto es mayor cuantos m√°s niveles tiene la variable original.\n",
        "* Al diluir la importancia de los predictores categ√≥ricos, estos tienen menos probabilidad de ser seleccionada por el modelo, lo que desvirt√∫a las m√©tricas que miden la importancia de los predictores.\n",
        "\n",
        "Por el momento, en scikit-learn es necesario hacer one-hot-encoding para convertir las variables categ√≥ricas en variables dummy si deseamos usar random forest. La implementaci√≥n de `H2O` s√≠ permite utilizar directamente variables categ√≥ricas.\n"
      ],
      "metadata": {
        "id": "SBmYIkPldr8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"steelblue\">Comparaci√≥n Random Forest y Gradient Boosting Trees</font>\n"
      ],
      "metadata": {
        "id": "D8QXxxOXf7BL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest se compara con frecuencia con otro algoritmo basado en √°rboles, Gradient Boosting Trees. Ambos m√©todos son muy potentes y la superioridad de uno u otro depende del problema al que se apliquen. Algunos aspectos a tener en cuenta son:\n",
        "\n",
        "* Gracias al out-of-bag error, el m√©todo de Random Forest no necesita recurrir a validaci√≥n cruzada para la optimizaci√≥n de sus hiperpar√°metros. Esto puede ser una ventaja muy importante cuando los requerimientos computacionales son limitantes. Esta caracter√≠stica tambi√©n est√° presente en Stochastic Gradient Boosting pero no en AdaBoost y Gradient Boosting.\n",
        "\n",
        "* Random forest tiene menos hiperpar√°metros, lo que hace m√°s sencillo su correcta implementaci√≥n.\n",
        "\n",
        "* Si existe una proporci√≥n alta de predictores irrelevantes, Random Forest puede verse perjudicado, sobre todo a medida que se reduce el n√∫mero de predictores ($ùëö$) evaluados. \n",
        "\n",
        "* En Random Forest, cada modelo que forma el ensemble es independiente del resto, esto permite que el entrenamiento sea paralelizable.\n",
        "\n",
        "* Random Forest no sufre problemas de overfitting por muchos √°rboles que se agreguen.\n",
        "\n",
        "* Si se realiza una buena optimizaci√≥n de hiperpar√°metros, Gradient Boosting Trees suele obtener resultados ligeramente superiores."
      ],
      "metadata": {
        "id": "gwWwXX-4gCD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"steelblue\">Aplicaciones</font>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jcjSMHVBgEaW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSwbRvx4cNAs"
      },
      "source": [
        "# <font color=\"steelblue\">Referencias y enlaces de inter√©s</font>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLedH8ovoFLj"
      },
      "source": [
        "√Årboles de decisi√≥n con Python: regresi√≥n y clasificaci√≥n by Joaqu√≠n Amat Rodrigo, available under a Attribution 4.0 International (CC BY 4.0) at https://www.cienciadedatos.net/documentos/py07_arboles_decision_python.html\n",
        "\n",
        "Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
        "\n",
        "GeÃÅron, A. (2017). Hands-on machine learning with Scikit-Learn and TensorFlow : concepts, tools, and techniques to build intelligent systems. Sebastopol, CA: O'Reilly Media. ISBN: 978-1491962299\n",
        "\n",
        "\n",
        "https://nyandwi.com/machine_learning_complete/\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}