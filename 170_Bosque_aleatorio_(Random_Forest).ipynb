{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7whhErFzY9J"
      },
      "source": [
        "# <font color=\"steelblue\">Algoritmos Bagging avanzados: Bosques aleatorios (Random Forest)</font>\n",
        "\n",
        "**Autoría**: \n",
        "\n",
        "*   Fernando Borrás (f.borras@umh.es)\n",
        "*   Federico Botella (federico@umh.es)\n",
        "*   Inés Hernández (ines.hernandezp@umh.es)\n",
        "*   Mª Asunción Martínez Mayoral (asun.mayoral@umh.es)\n",
        "*   Josep Moltó (j.molto@umh.es)\n",
        "*   Javier Morales (j.morales@umh.es) \n",
        "\n",
        "Departamento de Estadística, Matemáticas e Informática. \n",
        "\n",
        "Universidad Miguel Hernández de Elche. \n",
        "\n",
        "\n",
        "**Financiación**: El material que aparece a continuación se ha desarrollado dentro del marco del proyecto UNIDIGITAL- IASAC.\n",
        "\n",
        "<small><img src=https://raw.githubusercontent.com/ia4legos/MachineLearning/main/images/IASAC-UMH.png width=\"450\" height=\"200\"></small>\n",
        "\n",
        "**Fecha última edición**: 26/05/2022\n",
        "\n",
        "**Licencia**: <small><a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br /></small>\n",
        "\n",
        "No olvides hacer una copia si deseas utilizarlo. Al usar estos contenidos, acepta nuestros términos de uso y nuestra política de privacidad. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"steelblue\">Configuración del cuaderno</font>\n",
        "\n",
        "Para garantizar la funcionalidad completa de este cuaderno, es preciso ejecutar la celda de código a continuación."
      ],
      "metadata": {
        "id": "O7Ih_r3AZeB0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCLJ7Ka9Rq35",
        "outputId": "b70f5427-556c-4122-8d2f-0314666182ab",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuaderno configurado\n"
          ]
        }
      ],
      "source": [
        "#@title <b><font color=\"steelblue\" size=\"+1\"> Configuración de cuaderno\n",
        "\n",
        "# Cargamos módulos\n",
        "from io import StringIO \n",
        "import sys\n",
        "import numpy as np      # importamos numpy como np\n",
        "import pandas as pd     # importamos pandas como pd\n",
        "import math             # importamos módulo para cáculos matemáticos\n",
        "\n",
        "# Esta línea configura matplotlib para mostrar las figuras incrustadas en el jupyter notebook\n",
        "# Configuraciónde entorno gráfico\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt # importamos matplotlib como plt\n",
        "import seaborn as sns # importamos seaborn como sns\n",
        "sns.set_style(\"ticks\")\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "class Capturing(list):\n",
        "    def __enter__(self):\n",
        "        self._stdout = sys.stdout\n",
        "        sys.stdout = self._stringio = StringIO()\n",
        "        return self\n",
        "    def __exit__(self, *args):\n",
        "        self.extend(self._stringio.getvalue().splitlines())\n",
        "        del self._stringio    # free up some memory\n",
        "        sys.stdout = self._stdout\n",
        "\n",
        "with Capturing() as output:\n",
        "    print('Comenzamos....')\n",
        "\n",
        "with Capturing(output) as output:\n",
        "    # Librerías\n",
        "    import os\n",
        "    !pip install jupyterquiz\n",
        "    from jupyterquiz import display_quiz\n",
        "    import json\n",
        "    import base64\n",
        "    # Lectura ficheros json\n",
        "    # A configurar pra cada cuaderno en función de las preguntas de autoevalaución\n",
        "    for i in range(1,7):\n",
        "      !wget {f\"https://raw.githubusercontent.com/ia4legos/MachineLearning/main/autoeval/auto_50_{i}.json\"}\n",
        "    print(\"Cuaderno configurado\")\n",
        "\n",
        "if output[-1]=='Cuaderno configurado':\n",
        "    print(output[-1])\n",
        "else: \n",
        "    print(output[:-2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt99G2Jk5jX4"
      },
      "source": [
        "# <font color=\"steelblue\">Introducción</font>\n",
        "\n",
        "**Descripción:** \n",
        "\n",
        "**Nivel de Formación:** \n",
        "\n",
        "**Recomendaciones antes de usarlo:** \n",
        "\n",
        "**Objetivos de aprendizaje:**\n",
        "\n",
        "* "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"steelblue\">Objetivos de aprendizaje</font>\n",
        "\n",
        "- "
      ],
      "metadata": {
        "id": "F4EcPF7MZ0pY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"steelblue\">Contenidos</font>\n",
        "\n",
        "1. "
      ],
      "metadata": {
        "id": "DsQSZxb8Z6-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"steelblue\">Introducción al bosque aleatorio (Random Forest)</font>"
      ],
      "metadata": {
        "id": "QAgaWSzDmSw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un modelo de **bosque aleatorio** está formado por un conjunto de árboles de decisión individuales, cada uno entrenado con una muestra ligeramente distinta de los datos de entrenamiento generada mediante bootstrapping. La predicción de una nueva observación se obtiene agregando las predicciones de todos los árboles individuales que forman el modelo.\n",
        "\n",
        "Muchos métodos predictivos generan modelos globales en los que disponemos de una única ecuación o modelo de predicción. Sin embargo, en situaciones complejas con múltiples predictores, que interaccionan entre ellos de forma compleja y no lineal, es muy difícil encontrar un modelo predictivo los suficientemente preciso. Como ya hemos visto anteriormente los árboles de decisión nos permiten obtener un modelo con el que odemos manejar de forma sencilla relaciones complejas entre las posibles predictoras\n",
        "\n",
        "Sin embargo, como ya hemos visto la utilización de los árboles de decisión no está exenta de dificultades y por ese motivo se introduce aqui el algortimo de bosque aleatorio que es un método de conjunto bagging que nos permite mejorar la capacidad predictiva de los árboles de decisión individuales.\n",
        "\n",
        "Entre la ventajas del suso de este tipo de algortimo podemos destacar:\n",
        "\n",
        "* Son capaces de seleccionar predictores de forma automática.\n",
        "* Pueden aplicarse a problemas de regresión y clasificación.\n",
        "* Al tratarse de métodos no paramétricos, no es necesario que se cumpla ningún tipo de distribución específica.\n",
        "* Por lo general, requieren mucha menos limpieza y pre procesado de los datos en comparación a otros métodos de aprendizaje estadístico (por ejemplo, no requieren estandarización).\n",
        "* No se ven muy influenciados por outliers.\n",
        "* Son muy útiles en la exploración de datos, permiten identificar de forma rápida y eficiente las variables (predictores) más importantes.\n",
        "* Gracias al Out-of-Bag Error puede estimarse su error de validación sin necesidad de recurrir a estrategias computacionalmente costosas como la validación cruzada. \n",
        "\n",
        "Entre las desvantajas podemos destacar:\n",
        "\n",
        "* Al combinar múltiples árboles, se pierde la interpretabilidad que tienen los modelos basados en un único árbol.\n",
        "* Cuando tratan con predictores continuos, pierden parte de su información al categorizarlas en el momento de la división de los nodos.\n",
        "* Por la forma de construcción de los árboles de decisión los predictores continuos o predictores cualitativos con muchos niveles tienen mayor probabilidad de contener, solo por azar, algún punto de corte óptimo, por lo que suelen verse favorecidos en la creación de los árboles.\n",
        "* No son capaces de extrapolar fuera del rango de los predictores observado en los datos de entrenamiento."
      ],
      "metadata": {
        "id": "sylDxIgKNxgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"steelblue\">Algoritmo de bosque aleatorio</font>\n",
        "\n",
        "Antes de presentar el algoritmos específico de bosque aleatorio es necesario conocer como funciona el proceso de bagging para un único árbol de decisión. Dicho algoritmo se organiza en tres pasos:\n",
        "\n",
        "1. Generar  𝐵  pseudo-training sets mediante bootstrapping a partir de la muestra de entrenamiento original.\n",
        "2. Entrenar un árbol con cada una de las  𝐵  muestras del paso 1. Cada árbol se crea sin apenas restricciones y no se somete a pruning, por lo que tiene varianza alta pero poco sesgo. En la mayoría de casos, la única regla de parada es el número mínimo de observaciones que deben tener los nodos terminales. El valor óptimo de este hiperparámetro puede obtenerse comparando el out of bag error o por validación cruzada.\n",
        "3. Para cada una de la muestras de validación, se obtiene la predicción en cada uno de los 𝐵  árboles. El valor final de la predicción se obtiene como la media de las  𝐵  predicciones en el caso de variables cuantitativas y como la clase predicha más frecuente (moda) para variables cualitativas.\n",
        "\n",
        "En el algoritmo descrito, el número de árboles creados no es un hiperparámetro crítico en cuanto a que, por mucho que se incremente el número, no se aumenta el riesgo de overfitting. Alcanzado un determinado número de árboles, la reducción de test error se estabiliza. A pesar de ello, cada árbol ocupa memoria, por lo que no conviene almacenar más de los necesarios.\n",
        "\n",
        "El algoritmo de Random Forest es una modificación del proceso de bagging anterior que consigue mejorar los resultados gracias a que considera árboles lo más independientes posible. \n",
        "\n",
        "Supóngase un set de datos en el que hay un predictor muy influyente, junto con otros moderadamente influyentes. En este escenario, todos o casi todos los árboles creados en el proceso de bagging estarán dominados por el mismo predictor y serán muy parecidos entre ellos. Como consecuencia de la alta correlación entre los árboles, el proceso de bagging apenas conseguirá disminuir la varianza y, por lo tanto, tampoco mejorar el modelo. Random forest evita este problema haciendo una selección aleatoria de  $𝑚$  predictores antes de evaluar cada división. De esta forma, un promedio de  $(𝑝−𝑚)/𝑝$  divisiones no contemplarán el predictor influyente, permitiendo que otros predictores puedan ser seleccionados. Añadiendo este paso extra se consigue decorrelacionar los árboles todavía más, con lo que su agregación consigue una mayor reducción de la varianza. Algunas recomndaciones para la selección de $m$ son:\n",
        "\n",
        "* La raíz cuadrada del número total de predictores para problemas de clasificación:  \n",
        "\n",
        "$$m \\approx \\sqrt{p}$$\n",
        "\n",
        "* Un tercio del número de predictores para problemas de regresión:\n",
        "\n",
        "$$m \\approx p/3$$  \n",
        "\n",
        "* Si los predictores están muy correlacionados, valores pequeños de  $𝑚$  consiguen mejores resultados."
      ],
      "metadata": {
        "id": "k6w4J7a_ZRIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"steelblue\">Predicción mediante bosque aleatorio</font>"
      ],
      "metadata": {
        "id": "nAyouPxJijef"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para realizar la predicción de un bosque aleatorio utilizamos el principio de bagging, de forma que, una vez determinamos el nod terminal al que es asinado la observación apredecir en cada uno de los árboles, utilizamos las observaciones contenidas en dicho nodo terminal para la predicción individidual de cada uno de ellos. Si estamos en un modeo de regresión obtnemos la media de todas las observaciones del nodo terminal en cada árbol, mientras que si estamos en un problema de clasificación actuamos mediante el voto por mayoría.\n",
        "\n",
        "Una vez obtenemos las predicciones individuales la predicción conjunta se obtiene a partir de la media o de la categoría más frecuente de todas ellas en función de que estemos en un problema de predicción o clasificación.\n",
        "\n",
        "Sin embargo, en los problemas de regresión la predicción de un árbol de regresión puede verse como una variante de vecinos cercanos en la que, solo las observaciones que forman parte del mismo nodo terminal que la observación predicha, tienen influencia. Siguiendo esta aproximación, la predicción del árbol se define como la media ponderada de todas las observaciones de entrenamiento, donde el peso de cada observación depende únicamente de si forma parte o no del mismo nodo terminal, es decir, definimos los pesos del árbol j como un vector de $n$ componentes donde cada una de las componenetes toma el valor $w_j = 1/n_j$  si la observación peetenece al nodo terminal $j$ con $n_j$ observaciones, y 0 en otro caso. Para el bosque aletorio esto equivale a la media ponderada de todas las observaciones, empleando como pesos  la media de los vectores de pesos de los $M$ árboles considerados, es decir,\n",
        "\n",
        "$$\\hat{w}=\\frac{\\sum_{i=1}^{M}  w_i}{M}$$\n",
        "\n",
        "$$y_{pred} = \\sum_{i=1}^n \\hat{w}_i y_i$$"
      ],
      "metadata": {
        "id": "LuIj5gYclK3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"steelblue\">Importancia de los predictores</font>"
      ],
      "metadata": {
        "id": "u1VVGDf0s--d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si bien es cierto que el bosque aleatorio consigue mejorar la capacidad predictiva en comparación a los modelos basados en un único árbol, esto tiene un coste asociado, la interpretabilidad del modelo se reduce. Al tratarse de una combinación de múltiples árboles, no es posible obtener una representación gráfica sencilla del modelo y no es inmediato identificar de forma visual que predictores son más importantes. Sin embargo, se han desarrollado nuevas estrategias para cuantificar la importancia de los predictores que hacen de los modelos de bosque aleatorio una herramienta muy potente, no solo para predecir, sino también para el análisis exploratorio. Dos de estas medidas son: importancia por permutación e impureza de nodos."
      ],
      "metadata": {
        "id": "he0PC7JDtLKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"steelblue\">Importancia por permutación</font>"
      ],
      "metadata": {
        "id": "YZXvVcm9uCvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifica la influencia que tiene cada predictor sobre una determinada métrica de evaluación del modelo (estimada por out-of-bag error o validación cruzada). El valor asociado con cada predictor se obtiene de la siguiente forma:\n",
        "\n",
        "1. Crear el conjunto de árboles que forman el modelo.\n",
        "\n",
        "2. Calcular una determinada métrica de error (mse, classification error, ...). Este es el valor de referencia ($𝑒𝑟𝑟_0$).\n",
        "\n",
        "3. Para cada predictor $𝑗$:\n",
        "\n",
        "  * Permutar en todos los árboles del modelo los valores del predictor $𝑗$ manteniendo el resto constante.\n",
        "\n",
        "  * Recalcular la métrica tras la permutación, llámese ($𝑒𝑟𝑟_𝑗$).\n",
        "\n",
        "  * Calcular el incremento en la métrica debido a la permutación del predictor $𝑗$\n",
        "\n",
        "$$\\%I_𝑗=100*\\frac{err_j-err_0}{err_0}$$\n",
        "\n",
        "Si el predictor permutado estaba contribuyendo al modelo, es de esperar que el modelo aumente su error, ya que se pierde la información que proporcionaba esa variable. El porcentaje en que se incrementa el error debido a la permutación del predictor $𝑗$  puede interpretarse como la influencia que tiene $𝑗$  sobre el modelo. Algo que suele llevar a confusiones es el hecho de que este incremento puede resultar negativo. Si la variable no contribuye al modelo, es posible que, al reorganizarla aleatoriamente, solo por azar, se consiga mejorar ligeramente el modelo, por lo que  $(𝑒𝑟𝑟_𝑗−𝑒𝑟𝑟_0)$ es negativo. A modo general, se puede considerar que estas variables tiene una importancia próxima a cero.\n",
        "\n",
        "Aunque esta estrategia suele ser la más recomendado, cabe tomar algunas precauciones en su interpretación. Lo que cuantifican es la influencia que tienen los predictores sobre el modelo, no su relación con la variable respuesta. ¿Por qué es esto tan importante? Supóngase un escenario en el que se emplea esta estrategia con la finalidad de identificar qué predictores están relacionados con el peso de una persona, y que dos de los predictores son: el índice de masa corporal (IMC) y la altura. Como IMC y altura están muy correlacionados entre sí (la información que aportan es redundante), cuando se permute uno de ellos, el impacto en el modelo será mínimo, ya que el otro aporta la misma información. Como resultado, estos predictores aparecerán como poco influyentes aun cuando realmente están muy relacionados con la variable respuesta. Una forma de evitar problemas de este tipo es, siempre que se excluyan predictores de un modelo, comprobar el impacto que tiene en su capacidad predictiva."
      ],
      "metadata": {
        "id": "HjrquR6quNt8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color=\"steelblue\">Incremento de la pureza de los nodos</font>"
      ],
      "metadata": {
        "id": "5m653uKJwHof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cuantifica el incremento total en la pureza de los nodos debido a divisiones en las que participa el predictor (promedio de todos los árboles). La forma de calcularlo es la siguiente: en cada división de los árboles, se registra el descenso conseguido en la medida empleada como criterio de división (índice Gini, mse entropía, ...). Para cada uno de los predictores, se calcula el descenso medio conseguido en el conjunto de árboles que forman el conjunto. Cuanto mayor sea este valor medio, mayor la contribución del predictor en el modelo."
      ],
      "metadata": {
        "id": "9lDInvB8wOxg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"steelblue\">Hiperparámetros relevantes en el bosque aleatorio</font>"
      ],
      "metadata": {
        "id": "pOX9FjMDybO6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Del conjunto de hiperparámetros que se pueden modificar en el bosque aleatorio los dos más interesantes son el número de árboles considerados y el númeo máximo de predictoras usadas en la construcción de cada árbol. \n",
        "\n",
        "Lo habitual es proceder de forma individual estudiando la influencia de cada uno de los hiperparámetros respecto de la capacidad predictiva del modelo utilizando el out of bag score (aunque se puede configurar el algoritmo para utilizar otra). Esas curvas de influencia nos permiten deteminar la evolución del error del modelo con respecto a es hiperparámetro y obtener así el conjunto óptimo de valores. \n",
        "\n",
        "Sin embargo, aunque el análisis individual de los hiperparámetros es útil para entender su impacto en el modelo e identificar rangos de interés, la búsqueda final no debe hacerse de forma secuencial, ya que cada hiperparámetro interacciona con los demás. Es preferible recurrir a grid search o random search para analizar varias combinaciones de hiperparámetros. Los dos métodos más habituales son el grid search basado en el out of bag o el grid search basado en validación cruzada."
      ],
      "metadata": {
        "id": "hHxDpD6TzFzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"steelblue\">Codificación de predictoras cualitativas</font>\n"
      ],
      "metadata": {
        "id": "Shoiye0tZucA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los modelos basados en árboles de decisión, entre ellos Random Forest, son capaces de utilizar predictores categóricos en su forma natural sin necesidad de convertirlos en variables dummy mediante one hot encoding. Sin embargo, en la práctica, depende de la implementación que tenga la librería o software utilizado. Esto tiene impacto directo en la estructura de los árboles generados y, en consecuencia, en los resultados predictivos del modelo y en la importancia calculada para los predictores.\n",
        "\n",
        "Entre las dificulatdes más relvantes al utilizar one hot encoding se pueden destacar:\n",
        "\n",
        "* El entrenamiento de los modelos es mas costos cuando se aplica one hotencoding debido al aumento de dimensionalidad al crear las nuevas variables dummy, obligando a que el algoritmo tenga que analizar muchos más puntos de división.\n",
        "* Al convertir una variable categórica en múltiples variables dummy su importancia queda diluida, dificultando que el modelo pueda aprender de ella y perdiendo así capacidad predictiva. Este efecto es mayor cuantos más niveles tiene la variable original.\n",
        "* Al diluir la importancia de los predictores categóricos, estos tienen menos probabilidad de ser seleccionada por el modelo, lo que desvirtúa las métricas que miden la importancia de los predictores.\n",
        "\n",
        "Por el momento, en scikit-learn es necesario hacer one-hot-encoding para convertir las variables categóricas en variables dummy si deseamos usar random forest. La implementación de `H2O` sí permite utilizar directamente variables categóricas.\n"
      ],
      "metadata": {
        "id": "SBmYIkPldr8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"steelblue\">Comparación Random Forest y Gradient Boosting Trees</font>\n"
      ],
      "metadata": {
        "id": "D8QXxxOXf7BL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest se compara con frecuencia con otro algoritmo basado en árboles, Gradient Boosting Trees. Ambos métodos son muy potentes y la superioridad de uno u otro depende del problema al que se apliquen. Algunos aspectos a tener en cuenta son:\n",
        "\n",
        "* Gracias al out-of-bag error, el método de Random Forest no necesita recurrir a validación cruzada para la optimización de sus hiperparámetros. Esto puede ser una ventaja muy importante cuando los requerimientos computacionales son limitantes. Esta característica también está presente en Stochastic Gradient Boosting pero no en AdaBoost y Gradient Boosting.\n",
        "\n",
        "* Random forest tiene menos hiperparámetros, lo que hace más sencillo su correcta implementación.\n",
        "\n",
        "* Si existe una proporción alta de predictores irrelevantes, Random Forest puede verse perjudicado, sobre todo a medida que se reduce el número de predictores ($𝑚$) evaluados. \n",
        "\n",
        "* En Random Forest, cada modelo que forma el ensemble es independiente del resto, esto permite que el entrenamiento sea paralelizable.\n",
        "\n",
        "* Random Forest no sufre problemas de overfitting por muchos árboles que se agreguen.\n",
        "\n",
        "* Si se realiza una buena optimización de hiperparámetros, Gradient Boosting Trees suele obtener resultados ligeramente superiores."
      ],
      "metadata": {
        "id": "gwWwXX-4gCD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color=\"steelblue\">Aplicaciones</font>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jcjSMHVBgEaW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSwbRvx4cNAs"
      },
      "source": [
        "# <font color=\"steelblue\">Referencias y enlaces de interés</font>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLedH8ovoFLj"
      },
      "source": [
        "Árboles de decisión con Python: regresión y clasificación by Joaquín Amat Rodrigo, available under a Attribution 4.0 International (CC BY 4.0) at https://www.cienciadedatos.net/documentos/py07_arboles_decision_python.html\n",
        "\n",
        "Scikit-learn: Machine Learning in Python, Pedregosa et al., JMLR 12, pp. 2825-2830, 2011.\n",
        "\n",
        "Géron, A. (2017). Hands-on machine learning with Scikit-Learn and TensorFlow : concepts, tools, and techniques to build intelligent systems. Sebastopol, CA: O'Reilly Media. ISBN: 978-1491962299\n",
        "\n",
        "\n",
        "https://nyandwi.com/machine_learning_complete/\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}